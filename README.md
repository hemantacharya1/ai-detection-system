# ğŸ§  AI Image Detection PoC

**Detecting Nano-Bananaâ€“Generated Images using Provenance + CNN Forensics**

## ğŸ“Œ Overview

This project is a proof-of-concept (PoC) system designed to detect whether an image was generated by Nano-Banana Pro (or similar modern generative models) or is a real photograph.

The system follows a multi-layer detection strategy that combines:

- Cryptographic provenance detection (when metadata is available)
- CNN-based forensic analysis (when metadata is missing or stripped)

The goal is robust detection under real-world conditions, including:

- direct AI image downloads
- screenshots
- image resizing or cropping
- metadata removal (e.g., WhatsApp sharing)

This project focuses on process, explainability, and robustness, not production-scale deployment.

## ğŸ—ï¸ System Architecture

```
Image Input
   â”‚
   â”œâ”€â”€ Layer A: Metadata / C2PA Provenance (Deterministic)
   â”‚       â”œâ”€ If AI provenance found â†’ AI (confidence = 1.0)
   â”‚       â””â”€ Else â†’ fallback
   â”‚
   â””â”€â”€ Layer B: CNN-based Forensic Detection (Probabilistic)
           â”œâ”€ Patch-based ResNet-18 inference
           â”œâ”€ Aggregation across patches
           â””â”€ Confidence score (AI vs Real)
```

## ğŸ” Layer A â€” Metadata & Provenance Detection

**Purpose:**
Detect explicit, cryptographically signed declarations that an image was AI-generated.

**How it works:**

- Uses C2PA (Content Provenance and Authenticity) manifests
- Parses signed provenance metadata (if present)
- Looks for explicit AI-generation claims (e.g., DALLÂ·E, Gemini, Firefly, diffusion models)

**Key properties:**

- Deterministic (no probability)
- Zero false positives
- High precision, low recall
- Works only when metadata is preserved

**Why this layer exists:**
If an image explicitly declares AI generation, no forensic inference is needed.

## ğŸ§ª Layer B â€” CNN-based Forensic Detection

**Purpose:**
Detect AI-generated images when metadata is missing, such as:

- screenshots
- re-uploads
- edited or resized images

### Model Details

- **Architecture:** ResNet-18
- **Input:** random image patches (224Ã—224)
- **Output:** AI probability per patch
- **Aggregation:** percentile-based (robust to mixed content)
- **Normalization:** disabled (empirically improved robustness)

### Why Patch-Based?

- AI artifacts are often local
- Improves robustness to cropping/resizing
- Prevents overfitting to global image structure

### Output

- Probabilistic confidence score
- Final AI / Real decision based on threshold

## ğŸ“ Project Structure

```
ai-image-detection/
â”‚
â”œâ”€â”€ api/                          # FastAPI application
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ inference/                    # Inference logic
â”‚   â””â”€â”€ layer_b_infer.py
â”‚
â”œâ”€â”€ models/                       # Trained model artifacts
â”‚   â”œâ”€â”€ layer_b_model.pt          # ResNet-18 trained weights
â”‚   â””â”€â”€ layer_b_config.json       # Inference configuration
â”‚
â”œâ”€â”€ dataset/                      # (Optional) Training data
â”‚   â”œâ”€â”€ ai/
â”‚   â””â”€â”€ real/
â”‚
â”œâ”€â”€ download_real_images.py       # Unsplash data collection
â”œâ”€â”€ generate_ai_images.py         # HF Nano-Banana dataset loader
â”œâ”€â”€ layer_b_training.ipynb        # Training notebook
â”œâ”€â”€ metadata_layer.py             # Layer A implementation
â”œâ”€â”€ test.py                       # Local inference test
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸš€ Running Inference (No Training Required)

### 1ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 2ï¸âƒ£ Run local test

```bash
python test.py
```

### 3ï¸âƒ£ Run API

```bash
uvicorn api.main:app --reload
```

Upload an image to `/detect` to receive:

- AI / Real decision
- confidence score
- explanation (which layer decided)

## ğŸ§  Model Portability

The CNN model is stored as a PyTorch state_dict (.pt file).

âœ” Can be used on any machine  
âœ” No retraining required  
âœ” GPU optional (CPU inference supported)

At runtime:

- Model architecture is recreated in code
- Trained weights are loaded
- Inference runs immediately

## ğŸ‹ï¸ Retraining the Model (Optional)

If you want to retrain or extend the model:

### Dataset

**AI images:**
Hugging Face dataset `bitmind/nano-banana` (HF token required)

**Real images:**
Unsplash API (API key required)

### Scripts

```bash
python generate_ai_images.py
python download_real_images.py
```

### Training

Open and run:

```
layer_b_training.ipynb
```

The notebook covers:

- image-level train/val split
- patch extraction
- model training
- evaluation (ROC-AUC, PR-AUC)
- image-level aggregation

## ğŸ“Š Evaluation Summary

- Patch-level ROC-AUC â‰ˆ 0.92
- Image-level ROC-AUC â‰ˆ ~1.0 on provided test images
- Robust to screenshots and resized images
- Designed to generalize beyond a single generator

## ğŸ”® Future Improvements

- Model ensembles (multiple CNN backbones)
- Training-free OOD detection
- Invisible watermark detection (e.g., SynthID)
- Larger and more diverse training data

## ğŸ§¾ Summary

This PoC demonstrates a practical, explainable approach to AI image detection using:

- deterministic provenance checks where possible
- learned forensic signals where necessary


